{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":67356,"databundleVersionId":8006601,"sourceType":"competition"}],"dockerImageVersionId":30716,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load   \n \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-11T17:55:39.512148Z","iopub.execute_input":"2024-06-11T17:55:39.512407Z","iopub.status.idle":"2024-06-11T17:55:41.441125Z","shell.execute_reply.started":"2024-06-11T17:55:39.512379Z","shell.execute_reply":"2024-06-11T17:55:41.440225Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/leash-BELKA/sample_submission.csv\n/kaggle/input/leash-BELKA/train.parquet\n/kaggle/input/leash-BELKA/test.parquet\n/kaggle/input/leash-BELKA/train.csv\n/kaggle/input/leash-BELKA/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install dask[complete] rdkit scikit-learn","metadata":{"execution":{"iopub.status.busy":"2024-06-11T17:55:48.750187Z","iopub.execute_input":"2024-06-11T17:55:48.751179Z","iopub.status.idle":"2024-06-11T17:59:41.320070Z","shell.execute_reply.started":"2024-06-11T17:55:48.751144Z","shell.execute_reply":"2024-06-11T17:59:41.318967Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7e91846ad1b0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/dask/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7e91846ad4e0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/dask/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7e91846ad930>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/dask/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7e91846adae0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/dask/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7e91846adc90>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/dask/\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement dask[complete] (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for dask[complete]\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install dask","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    from rdkit import Chem\n    from rdkit.Chem import AllChem\n    print(\"RDKit is available.\")\nexcept ImportError:\n    print(\"RDKit is not available.\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Import Libraries and Define File Path**","metadata":{}},{"cell_type":"code","source":"import dask.dataframe as dd\nfrom dask.delayed import delayed\nfrom rdkit import Chem\nfrom rdkit.Chem import AllChem\nfrom sklearn.preprocessing import OneHotEncoder\nimport pandas as pd\nimport numpy as np\nimport os\nimport uuid\n\n# Define the file path and columns to use\nfile_path = '/kaggle/input/leash-BELKA/train.parquet'\ndask_df = dd.read_parquet(file_path)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Load Data and Limit Samples**","metadata":{}},{"cell_type":"code","source":"def limit_data(df, n_samples_per_class=550000):\n    total_binders = df[df['binds'] == 1].shape[0].compute()\n    total_non_binders = df[df['binds'] == 0].shape[0].compute()\n    \n    print(f\"Total binders: {total_binders}, Total non-binders: {total_non_binders}\")\n    \n    # Ensure n_samples_per_class does not exceed the actual number of samples available\n    n_samples_per_class = min(n_samples_per_class, total_binders, total_non_binders)\n    \n    binders = df[df['binds'] == 1].sample(frac=n_samples_per_class / total_binders, random_state=42)\n    non_binders = df[df['binds'] == 0].sample(frac=n_samples_per_class / total_non_binders, random_state=42)\n    \n    return dd.concat([binders, non_binders])\n\nlimited_df = limit_data(dask_df)\n\n# Debug: Check the size of the limited data\nprint(\"Size of limited_df:\", limited_df.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Process each Chunk**","metadata":{}},{"cell_type":"code","source":"def process_chunk(chunk):\n    chunk['molecule'] = chunk['molecule_smiles'].apply(Chem.MolFromSmiles)\n    chunk['ecfp'] = chunk['molecule'].apply(lambda mol: list(AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=1024)) if mol else [0]*1024)\n    \n    # Convert the protein_name column to a NumPy array\n    protein_names = chunk['protein_name'].to_numpy()\n    \n    onehot_encoder = OneHotEncoder(sparse_output=False)\n    if len(protein_names) > 0:\n        protein_onehot = onehot_encoder.fit_transform(protein_names.reshape(-1, 1))\n    else:\n        protein_onehot = np.array([]).reshape(0, 0)\n    \n    # Combine ECFPs and one-hot encoded protein_name\n    combined_features = [ecfp + list(protein) for ecfp, protein in zip(chunk['ecfp'].tolist(), protein_onehot.tolist())]\n    \n    # Convert combined_features to strings\n    combined_features_str = [','.join(map(str, features)) for features in combined_features]\n    \n    return pd.DataFrame({'id': chunk['id'], 'features': combined_features_str, 'binds': chunk['binds']})\n\n# Apply the processing function to each chunk\nprocessed_df = limited_df.map_partitions(process_chunk, meta={'id': 'int64', 'features': 'object', 'binds': 'int64'})\n\n# Generate a unique file name\nunique_file_name = f'/kaggle/working/processed_dataset_{uuid.uuid4()}.parquet'\n\n# Persist the processed DataFrame to disk\nprocessed_df.to_parquet(unique_file_name, write_index=False, compression='snappy', engine='pyarrow')\n\nprint(f\"Data processing complete. The processed file is saved as '{unique_file_name}'.\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Load Processed Data**","metadata":{}},{"cell_type":"code","source":"print(\"something\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Process Data and Write to Parquet**","metadata":{}},{"cell_type":"code","source":"# Load Processed Data\nprocessed_df = pd.read_parquet(unique_file_name)\n\n# Convert the features back to lists of floats\nprocessed_df['features'] = processed_df['features'].apply(lambda x: list(map(float, x.split(','))))\n\n# Debug: Check if processed_df is not empty\nprint(\"Size of processed_df:\", processed_df.shape)\nprint(processed_df.head())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prepare Data for Model Train ing\n# Ensure all feature vectors have the same length\nmax_length = max(processed_df['features'].apply(len))\n\ndef pad_features(features, max_length):\n    return features + [0.0] * (max_length - len(features))\n\nprocessed_df['features'] = processed_df['features'].apply(lambda x: pad_features(x, max_length))\n\nX = np.array(processed_df['features'].tolist())\ny = processed_df['binds'].tolist()\n\n# Debug: Check the shapes of X and y\nprint(\"Shape of X:\", X.shape)\nprint(\"Shape of y:\", len(y))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Prepare Data for Model Training**","metadata":{}},{"cell_type":"markdown","source":"**Split Data into Training and Testing Sets**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Debug: Check the shapes of the splits\nprint(\"Shape of X_train:\", X_train.shape)\nprint(\"Shape of X_test:\", X_test.shape)\nprint(\"Shape of y_train:\", len(y_train))\nprint(\"Shape of y_test:\", len(y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train the Random Forest Model**","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n# Create and train the random forest model\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(X_train, y_train)\n\n# Debug: Check if the model is trained\nprint(\"Model training completed.\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Make Predictions and Evaluate the Model**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import average_precision_score, classification_report, confusion_matrix\n\n# Make predictions on the test set\ny_pred_proba = rf_model.predict_proba(X_test)[:, 1]  # Probability of the positive class\n\n# Calculate the mean average precision\nmap_score = average_precision_score(y_test, y_pred_proba)\nprint(f\"Mean Average Precision (mAP): {map_score:.2f}\")\n\n# Make binary predictions\ny_pred = rf_model.predict(X_test)\n\n# Print classification report\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\n\n# Print confusion matrix\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Save the Trained Model (Optional)**","metadata":{}},{"cell_type":"code","source":"import joblib\n\n# Save the trained model to a file\nmodel_path = 'random_forest_model.pkl'\njoblib.dump(rf_model, model_path)\nprint(f\"Model saved to {model_path}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Split Data into Training and Testing Sets**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Debug: Check the shapes of the splits\nprint(\"Shape of X_train:\", X_train.shape)\nprint(\"Shape of X_test:\", X_test.shape)\nprint(\"Shape of y_train:\", len(y_train))\nprint(\"Shape of y_test:\", len(y_test))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train the Random Forest Model**","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n# Create and train the random forest model\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(X_train, y_train)\n\n# Debug: Check if the model is trained\nprint(\"Model training completed.\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Make Predictions and Evaluate the Model**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import average_precision_score, classification_report, confusion_matrix\n\n# Make predictions on the test set\ny_pred_proba = rf_model.predict_proba(X_test)[:, 1]  # Probability of the positive class\n\n# Calculate the mean average precision\nmap_score = average_precision_score(y_test, y_pred_proba)\nprint(f\"Mean Average Precision (mAP): {map_score:.2f}\")\n\n# Make binary predictions\ny_pred = rf_model.predict(X_test)\n\n# Print classification report\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\n\n# Print confusion matrix\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Save the Trained Model (Optional)**","metadata":{}},{"cell_type":"code","source":"import joblib\n\n# Save the trained model to a file\nmodel_path = 'random_forest_model.pkl'\njoblib.dump(rf_model, model_path)\nprint(f\"Model saved to {model_path}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**starting with test data**","metadata":{}},{"cell_type":"markdown","source":"Load the Test Data","metadata":{}},{"cell_type":"code","source":"import dask.dataframe as dd\n\n# Define the file path for test data\ntest_path = '/kaggle/input/leash-BELKA/test.parquet'\n\n# Load the Parquet file using Dask\ndask_test_df = dd.read_parquet(test_path)\n\n# Display the shape of the test data\nprint(f\"Shape of test data: {dask_test_df.shape}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Process Each Chunk","metadata":{}},{"cell_type":"code","source":"from rdkit import Chem\nfrom rdkit.Chem import AllChem\nfrom sklearn.preprocessing import OneHotEncoder\nimport pandas as pd\nimport numpy as np\n\n# Define a function to process each chunk\ndef process_test_chunk(chunk):\n    chunk['molecule'] = chunk['molecule_smiles'].apply(Chem.MolFromSmiles)\n    chunk['ecfp'] = chunk['molecule'].apply(lambda mol: list(AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=1024)) if mol else [0]*1024)\n    \n    # Convert the protein_name column to a NumPy array\n    protein_names = chunk['protein_name'].to_numpy()\n    \n    onehot_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n    if len(protein_names) > 0:\n        protein_onehot = onehot_encoder.fit_transform(protein_names.reshape(-1, 1))\n    else:\n        protein_onehot = np.array([]).reshape(0, 0)\n    \n    # Combine ECFPs and one-hot encoded protein_name\n    combined_features = [ecfp + list(protein) for ecfp, protein in zip(chunk['ecfp'].tolist(), protein_onehot.tolist())]\n    \n    # Convert combined_features to strings\n    combined_features_str = [','.join(map(str, features)) for features in combined_features]\n    \n    return pd.DataFrame({'id': chunk['id'], 'features': combined_features_str})\n\n# Apply the processing function to each chunk\nprocessed_test_df = dask_test_df.map_partitions(process_test_chunk, meta={'id': 'int64', 'features': 'object'})\n\n# Persist the processed DataFrame to disk\nprocessed_test_df.to_parquet('processed_test_dataset.parquet', write_index=False, compression='snappy', engine='pyarrow')\n\nprint(\"Test data processing complete. The processed test file is saved as 'processed_test_dataset.parquet'.\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Load and Verify Processed Data**","metadata":{}},{"cell_type":"code","source":"# Load the processed test data\nprocessed_test_df = pd.read_parquet('processed_test_dataset.parquet')\n\n# Display the shape of the processed test data\nprint(f\"Shape of processed test data: {processed_test_df.shape}\")\n\n# Display the first few rows of the processed test data\nprint(processed_test_df.head())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Prepare Features for Model Prediction**","metadata":{}},{"cell_type":"code","source":"# Convert the features back to lists of floats\nprocessed_test_df['features'] = processed_test_df['features'].apply(lambda x: list(map(float, x.split(','))))\n\n# Debug: Check if processed_test_df is not empty\nprint(\"Size of processed_test_df:\", processed_test_df.shape)\nprint(processed_test_df.head())\n\n# Ensure all feature vectors have the same length\nmax_length = max(processed_test_df['features'].apply(len))\n\ndef pad_features(features, max_length):\n    return features + [0.0] * (max_length - len(features))\n\nprocessed_test_df['features'] = processed_test_df['features'].apply(lambda x: pad_features(x, max_length))\n\n# Convert the processed DataFrame to a NumPy array for prediction\nX_test = np.array(processed_test_df['features'].tolist())\ntest_ids = processed_test_df['id'].values\n\n# Display the shape of the features\nprint(f\"Shape of X_test: {X_test.shape}\")\nprint(f\"Shape of test_ids: {test_ids.shape}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Load the Trained Model and Make Predictions**","metadata":{}},{"cell_type":"code","source":"import joblib\n\n# Load the trained model\nrf_model = joblib.load('/kaggle/working/random_forest_model.pkl')\n\n# Make predictions on the test set\ny_test_pred_proba = rf_model.predict_proba(X_test)[:, 1]  # Probability of the positive class\n\n# Prepare the submission DataFrame\nsubmission_df = pd.DataFrame({'id': test_ids, 'prediction': y_test_pred_proba})\n\n# Save the submission to a CSV file\nsubmission_df.to_csv('/kaggle/working/test_predictions.csv', index=False)\n\nprint(\"Test predictions saved to 'test_predictions.csv'.\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}